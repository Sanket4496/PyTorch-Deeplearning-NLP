{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation \n",
    "## Byte Pair Encoding\n",
    "Ref:[Nural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162.pdf)\n",
    "\n",
    "\n",
    "Byte pair encoding ([Gage 1994](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)) was initially developed as an algorithm to encode/compress a text based on the **most frequently occuring bytes**(a byte or 8 bits refers to a single character token for practical usage). \n",
    "\n",
    "The algorithm merges most frequently occuring consecutive bytes and replaces them with a new representative token that is not part of the existing lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    # Initialize a defaultdict to count pairs of consecutive symbols\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    # Iterate over each word and its frequency in the vocabulary\n",
    "    for word, freq in vocab.items():\n",
    "        # Split the word into symbols (assuming space-separated symbols)\n",
    "        symbols = word.split()\n",
    "        # Iterate over the symbols to count pairs of consecutive symbols\n",
    "        for i in range(len(symbols)-1):\n",
    "            # Increment the count of the pair by the frequency of the word\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "            \n",
    "    # Return the dictionary of pairs and their frequencies\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    # Initilize an empty dictionary for the new vocabulary\n",
    "    v_out = {}\n",
    "    # Create a bigram string from the pair, esacping any special characters\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    # Compile a regular expression pattern to match the bigram as a whole word\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    # Iterate over each word in the input vocabulary\n",
    "    for word in v_in:\n",
    "        # Replace occurances of the bigram with the merged pair\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        # Update the new vocabulary with the modified word and its frequency\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    # Return the new vocabulary\n",
    "    return v_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_learner_bpe(vocab, num_merges):\n",
    "    merge_rules = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        merge_rules.append(best)\n",
    "    return merge_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_segmenter_bpe(test_word, merge_rules):\n",
    "    # Split into characters and add end-of-word symbol\n",
    "    symbols = list(test_word) + ['</w>']\n",
    "    while True:\n",
    "        pairs = [(symbols[i], symbols[i+1]) for i in range(len(symbols) - 1)]\n",
    "        candidates = [pair for pair in pairs if pair in merge_rules]\n",
    "        if not candidates:\n",
    "            break\n",
    "        best_pair = min(candidates, key=merge_rules.index)\n",
    "        i = pairs.index(best_pair)\n",
    "        symbols = symbols[:i] + [''.join(best_pair)] + symbols[i + 2:]\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Merge Rules:\n",
      "[('e', 's'), ('es', 't'), ('est', '</w>'), ('l', 'o'), ('lo', 'w'), ('n', 'e'), ('ne', 'w'), ('new', 'est</w>'), ('low', '</w>'), ('w', 'i'), ('wi', 'd'), ('wid', 'est</w>'), ('low', 'e'), ('lowe', 'r'), ('lower', '</w>')]\n",
      "\n",
      "Segmented Test Data:\n",
      "lowest -> low est</w>\n",
      "newest -> newest</w>\n",
      "widest -> widest</w>\n"
     ]
    }
   ],
   "source": [
    "# Training Vocabulary\n",
    "train_vocab = {'l o w </w>': 5, 'l o w e r </w>': 2,\n",
    "               'n e w e s t </w>': 6, 'w i d e s t </w>': 3}\n",
    "# Learn merge rules\n",
    "num_merges = 15\n",
    "merge_rules = token_learner_bpe(train_vocab, num_merges)\n",
    "print(\"Learned Merge Rules:\")\n",
    "print(merge_rules)\n",
    "\n",
    "# Test the segmenter on new data\n",
    "test_data = ['lowest', 'newest', 'widest']\n",
    "print(\"\\nSegmented Test Data:\")\n",
    "for word in test_data:\n",
    "    segmented_word = token_segmenter_bpe(word, merge_rules)\n",
    "    print(f\"{word} -> {' '.join(segmented_word)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
